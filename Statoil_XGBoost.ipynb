{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scikit learn ensembe workflow for binary probability\n",
    "import time; start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random; random.seed(2016)\n",
    "from math import*\n",
    "from decimal import Decimal\n",
    "import os\n",
    "from scipy import fftpack\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_intersection(h1, h2, bins):\n",
    "   bins = np.diff(bins)\n",
    "   sm = 0\n",
    "   for i in range(len(bins)):\n",
    "       sm += min(bins[i]*h1[i], bins[i]*h2[i])\n",
    "   return sm\n",
    "\n",
    "def manhattan_distance(x,y):\n",
    "    return sum(abs(a-b) for a,b in zip(x,y))\n",
    " \n",
    "def nth_root(value, n_root):\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    " \n",
    "def minkowski_distance(x,y,p_value):\n",
    "     return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value)\n",
    "    \n",
    "def square_rooted(x):\n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)    \n",
    "\n",
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "def fft(img):\n",
    "    # Take the fourier transform of the image.\n",
    "    F1 = fftpack.fft2(img)\n",
    " \n",
    "    # Now shift the quadrants around so that low spatial frequencies are in\n",
    "    # the center of the 2D fourier transformed image.\n",
    "    F2 = fftpack.fftshift( F1 )\n",
    " \n",
    "    # Calculate a 2D power spectrum\n",
    "    psd2D = np.abs( F2 )**2\n",
    "    return psd2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "train = pd.read_json(\"./train.json\")\n",
    "test = pd.read_json(\"./test.json\")\n",
    "\n",
    "# train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "metrics = []\n",
    "\n",
    "if os.path.isfile('./train_metrics.csv'):\n",
    "    print 'Already done...'\n",
    "else:\n",
    "    for idx in range(0,train.shape[0]):\n",
    "        row = []\n",
    "        img1 = train.loc[idx, ['band_1', 'band_2']]\n",
    "        target = (train.loc[idx, ['is_iceberg']])[0]\n",
    "        row.append(target)\n",
    "        \n",
    "        id = (train.loc[idx, ['id']])[0]\n",
    "        row.append(id)\n",
    "\n",
    "        img1 = np.stack([img1['band_1'], img1['band_2']], -1).reshape(75, 75, 2)\n",
    "        band1 = img1[:, :, 0]\n",
    "        band1 = (band1 + abs(band1.min())) / np.max((band1 + abs(band1.min())))\n",
    "\n",
    "        band2 = img1[:, :, 1]\n",
    "        band2 = (band2 + abs(band2.min())) / np.max(band2 + abs(band2.min()))\n",
    "\n",
    "        fft_band1 = fft(band1)\n",
    "        fft_band2 = fft(band2)\n",
    "\n",
    "        hband1, hbins = np.histogram(band1, bins=256, normed=True)\n",
    "        hband2, hbins = np.histogram(band2, bins=256, normed=True)\n",
    "\n",
    "        fft_hband1, hbins = np.histogram(fft_band1, bins=256, normed=True)\n",
    "        fft_hband2, hbins = np.histogram(fft_band2, bins=256, normed=True)\n",
    "\n",
    "        hist_band1 = np.array([hband1]).ravel()\n",
    "        series_band1 = pd.Series(hist_band1)\n",
    "        hist_band2 = np.array([hband2]).ravel()\n",
    "        series_band2 = pd.Series(hist_band2)\n",
    "\n",
    "        fft_hist_band1 = np.array([fft_hband1]).ravel()\n",
    "        fft_series_band1 = pd.Series(fft_hist_band1)\n",
    "        fft_hist_band2 = np.array([fft_hband2]).ravel()\n",
    "        fft_series_band2 = pd.Series(fft_hist_band2)\n",
    "\n",
    "        diff = series_band1 - series_band2\n",
    "        distance = np.sqrt(np.dot(diff, diff))\n",
    "        row.append(distance)\n",
    "\n",
    "        inter = histogram_intersection(hband1, hband2, hbins)\n",
    "        row.append(inter)\n",
    "\n",
    "        diff = fft_series_band1 - fft_series_band2\n",
    "        distance = np.sqrt(np.dot(diff, diff))\n",
    "        row.append(distance)\n",
    "\n",
    "        inter = histogram_intersection(fft_hband1, fft_hband2, hbins)\n",
    "        row.append(inter)\n",
    "\n",
    "        manhattan = manhattan_distance(np.array([band1]).ravel(),np.array([band2]).ravel())\n",
    "        row.append(manhattan)\n",
    "\n",
    "        minkowski = minkowski_distance(np.array([band1]).ravel(),np.array([band2]).ravel(), 3)\n",
    "        row.append(minkowski)\n",
    "\n",
    "        cos_simil = cosine_similarity(np.array([band1]).ravel(),np.array([band2]).ravel())\n",
    "        row.append(cos_simil)\n",
    "\n",
    "        jaccard_simil = jaccard_similarity(np.array([band1]).ravel(),np.array([band2]).ravel())\n",
    "        row.append(jaccard_simil)\n",
    "        \n",
    "        metrics.append(row)\n",
    "        \n",
    "header = 'target', 'id', 'normal_dist','normal_intersec', 'fft_dist','fft_intersec', 'manhattan_dist', 'minkowski_dist', 'cosine_simil', 'jaccard_simil'\n",
    "            \n",
    "with open(\"./train_metrics.csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "metrics = []\n",
    "\n",
    "if os.path.isfile('./test_metrics.csv'):\n",
    "    print 'Already done...'\n",
    "else:\n",
    "    for idx in range(0,train.shape[0]):\n",
    "        row = []\n",
    "        img1 = train.loc[idx, ['band_1', 'band_2']]\n",
    "        \n",
    "        row.append(0)\n",
    "        \n",
    "        id = (train.loc[idx, ['id']])[0]\n",
    "        row.append(id)\n",
    "        \n",
    "        img1 = np.stack([img1['band_1'], img1['band_2']], -1).reshape(75, 75, 2)\n",
    "        band1 = img1[:, :, 0]\n",
    "        band1 = (band1 + abs(band1.min())) / np.max((band1 + abs(band1.min())))\n",
    "\n",
    "        band2 = img1[:, :, 1]\n",
    "        band2 = (band2 + abs(band2.min())) / np.max(band2 + abs(band2.min()))\n",
    "\n",
    "        fft_band1 = fft(band1)\n",
    "        fft_band2 = fft(band2)\n",
    "\n",
    "        hband1, hbins = np.histogram(band1, bins=256, normed=True)\n",
    "        hband2, hbins = np.histogram(band2, bins=256, normed=True)\n",
    "\n",
    "        fft_hband1, hbins = np.histogram(fft_band1, bins=256, normed=True)\n",
    "        fft_hband2, hbins = np.histogram(fft_band2, bins=256, normed=True)\n",
    "\n",
    "        hist_band1 = np.array([hband1]).ravel()\n",
    "        series_band1 = pd.Series(hist_band1)\n",
    "        hist_band2 = np.array([hband2]).ravel()\n",
    "        series_band2 = pd.Series(hist_band2)\n",
    "\n",
    "        fft_hist_band1 = np.array([fft_hband1]).ravel()\n",
    "        fft_series_band1 = pd.Series(fft_hist_band1)\n",
    "        fft_hist_band2 = np.array([fft_hband2]).ravel()\n",
    "        fft_series_band2 = pd.Series(fft_hist_band2)\n",
    "\n",
    "        diff = series_band1 - series_band2\n",
    "        distance = np.sqrt(np.dot(diff, diff))\n",
    "        row.append(distance)\n",
    "\n",
    "        inter = histogram_intersection(hband1, hband2, hbins)\n",
    "        row.append(inter)\n",
    "\n",
    "        diff = fft_series_band1 - fft_series_band2\n",
    "        distance = np.sqrt(np.dot(diff, diff))\n",
    "        row.append(distance)\n",
    "\n",
    "        inter = histogram_intersection(fft_hband1, fft_hband2, hbins)\n",
    "        row.append(inter)\n",
    "\n",
    "        manhattan = manhattan_distance(np.array([band1]).ravel(),np.array([band2]).ravel())\n",
    "        row.append(manhattan)\n",
    "\n",
    "        minkowski = minkowski_distance(np.array([band1]).ravel(),np.array([band2]).ravel(), 3)\n",
    "        row.append(minkowski)\n",
    "\n",
    "        cos_simil = cosine_similarity(np.array([band1]).ravel(),np.array([band2]).ravel())\n",
    "        row.append(cos_simil)\n",
    "\n",
    "        jaccard_simil = jaccard_similarity(np.array([band1]).ravel(),np.array([band2]).ravel())\n",
    "        row.append(jaccard_simil)\n",
    "        \n",
    "        metrics.append(row)\n",
    "        \n",
    "header = 'target', 'id', 'normal_dist','normal_intersec', 'fft_dist','fft_intersec', 'manhattan_dist', 'minkowski_dist', 'cosine_simil', 'jaccard_simil'\n",
    "            \n",
    "with open(\"./test_metrics.csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_metrics.csv')\n",
    "test = pd.read_csv('./test_metrics.csv')\n",
    "num_train = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "train = train.drop(['target'],axis=1)\n",
    "id_test = test['ID']\n",
    "\n",
    "def fill_nan_null(val):\n",
    "    ret_fill_nan_null = 0.0\n",
    "    if val == True:\n",
    "        ret_fill_nan_null = 1.0\n",
    "    return ret_fill_nan_null\n",
    "\n",
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "df_all['null_count'] = df_all.isnull().sum(axis=1).tolist()\n",
    "df_all_temp = df_all['ID']\n",
    "df_all = df_all.drop(['ID'],axis=1)\n",
    "df_data_types = df_all.dtypes[:] #{'object':0,'int64':0,'float64':0,'datetime64':0}\n",
    "d_col_drops = []\n",
    "\n",
    "for i in range(len(df_data_types)):\n",
    "    df_all[str(df_data_types.index[i])+'_nan_'] = df_all[str(df_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))\n",
    "df_all = df_all.fillna(-9999)\n",
    "#df_all = df_all.replace(0, -9999)\n",
    "\n",
    "for i in range(len(df_data_types)):\n",
    "    if str(df_data_types[i])=='object':\n",
    "        df_u = pd.unique(df_all[str(df_data_types.index[i])].ravel())\n",
    "        print(\"Column: \", str(df_data_types.index[i]), \" Length: \", len(df_u))\n",
    "        d={}\n",
    "        j = 1000\n",
    "        for s in df_u:\n",
    "            d[str(s)]=j\n",
    "            j+=5\n",
    "        df_all[str(df_data_types.index[i])+'_vect_'] = df_all[str(df_data_types.index[i])].map(lambda x:d[str(x)])\n",
    "        d_col_drops.append(str(df_data_types.index[i]))\n",
    "        if len(df_u)<150:\n",
    "            dummies = pd.get_dummies(df_all[str(df_data_types.index[i])]).rename(columns=lambda x: str(df_data_types.index[i]) + '_' + str(x))\n",
    "            df_all_temp = pd.concat([df_all_temp, dummies], axis=1)\n",
    "\n",
    "df_all_temp = df_all_temp.drop(['ID'],axis=1)\n",
    "df_all = pd.concat([df_all, df_all_temp], axis=1)\n",
    "print(len(df_all), len(df_all.columns))\n",
    "#df_all.to_csv(\"df_all.csv\")\n",
    "train = df_all.iloc[:num_train]\n",
    "test = df_all.iloc[num_train:]\n",
    "train = train.drop(d_col_drops,axis=1)\n",
    "test = test.drop(d_col_drops,axis=1)\n",
    "\n",
    "def flog_loss(ground_truth, predictions):\n",
    "    flog_loss_ = log_loss(ground_truth, predictions) #, eps=1e-15, normalize=True, sample_weight=None)\n",
    "    return flog_loss_\n",
    "LL  = make_scorer(flog_loss, greater_is_better=False)\n",
    "\n",
    "g={'ne':150,'md':6,'mf':80,'rs':2016} #change to g={'ne':500,'md':40,'mf':60,'rs':2016}\n",
    "etc = ensemble.ExtraTreesClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], criterion='entropy', min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)      \n",
    "etr = ensemble.ExtraTreesRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)      \n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], criterion='entropy', min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "xgr = xgb.XGBRegressor(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, objective='reg:linear')\n",
    "xgc = xgb.XGBClassifier(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, objective='binary:logistic') #try 'binary:logitraw'\n",
    "#clf = {'etc':etc, 'etr':etr, 'rfc':rfc, 'rfr':rfr, 'xgr':xgr, 'xgc':xgc} # use this line instead\n",
    "clf = {'etr':etr, 'rfr':rfr, 'xgr':xgr} # removed due to kaggle performance, would prefer less time and more cores than more time and less cores :)\n",
    "\n",
    "y_pred=[]\n",
    "best_score = 0.0\n",
    "id_results = id_test[:]\n",
    "for c in clf:\n",
    "    if c[:1] != \"x\": #not xgb\n",
    "        model = GridSearchCV(estimator=clf[c], param_grid={}, n_jobs =-1, cv=2, verbose=0, scoring=LL)\n",
    "        model.fit(train, y_train.values)\n",
    "        if c[-1:] != \"c\": #not classifier\n",
    "            y_pred = model.predict(test)\n",
    "            print(\"Ensemble Model: \", c, \" Best CV score: \", model.best_score_, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        else: #classifier\n",
    "            best_score = (log_loss(y_train.values, model.predict_proba(train)))*-1\n",
    "            y_pred = model.predict_proba(test)[:,1]\n",
    "            print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "    else: #xgb\n",
    "        X_fit, X_eval, y_fit, y_eval= train_test_split(train, y_train, test_size=0.35, train_size=0.65, random_state=g['rs'])\n",
    "        model = clf[c]\n",
    "        model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "        if c == \"xgr\": #xgb regressor\n",
    "            best_score = (log_loss(y_train.values, model.predict(train)))*-1\n",
    "            y_pred = model.predict(test)\n",
    "        else: #xgb classifier\n",
    "            best_score = (log_loss(y_train.values, model.predict_proba(train)))*-1\n",
    "            y_pred = model.predict_proba(test)[:,1]\n",
    "        print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]<0.0:\n",
    "            y_pred[i] = 0.0\n",
    "        if y_pred[i]>1.0:\n",
    "            y_pred[i] = 1.0\n",
    "    df_in = pd.DataFrame({\"ID\": id_test, c: y_pred})\n",
    "    id_results = pd.concat([id_results, df_in[c]], axis=1)\n",
    "id_results['avg'] = id_results.drop('ID', axis=1).apply(np.average, axis=1)\n",
    "id_results['min'] = id_results.drop('ID', axis=1).apply(min, axis=1)\n",
    "id_results['max'] = id_results.drop('ID', axis=1).apply(max, axis=1)\n",
    "id_results['diff'] = id_results['max'] - id_results['min']\n",
    "for i in range(10):\n",
    "    print(i, len(id_results[id_results['diff']>(i/10)]))\n",
    "id_results.to_csv(\"results_analysis.csv\", index=False)\n",
    "ds = id_results[['ID','avg']]\n",
    "ds.columns = ['ID','PredictedProb']\n",
    "ds.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
